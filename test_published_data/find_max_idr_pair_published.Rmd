---
title: "using IDR peaks"
output: html_notebook
---


# RUN THIS IN THE TEST_PUBLISHED_DATA DIRECTORY

# Skip for concat idr peaks
```{r}

hlow_idr_peaks = list.files(path="./idr_results/H3k27me3/H1low/", pattern = "IDR.*broadPeak$", full.names = TRUE )
print(hlow_idr_peaks)

# now find which broadPeak file has the most lines ( the most amount of peaks called)

hlow_peak_counts = data.frame(file = hlow_idr_peaks,
                            peak_counts = sapply(hlow_idr_peaks, function(x) {length(readLines(x))} )
                            
                            )

hlow_peak_counts

best_hlow_idr = hlow_peak_counts[which.max(hlow_peak_counts$peak_counts),]$file

# now this gives the file with the most peaks as according to encode pipeline
best_hlow_idr


# now doing the same for scrm

scrm_idr_peaks = list.files(path="./idr_results/H3k27me3/Scrm/", pattern = "IDR.*broadPeak$", full.names = TRUE )
print(scrm_idr_peaks)

# now find which broadPeak file has the most lines ( the most amount of peaks called)

scrm_peak_counts = data.frame(file = scrm_idr_peaks,
                            peak_counts = sapply(scrm_idr_peaks, function(x) {length(readLines(x))} )
                            
                            )

scrm_peak_counts

best_scrm_idr = scrm_peak_counts[which.max(scrm_peak_counts$peak_counts),]$file
```


# start from here with the concat 0.4 idr peaks since you dont have to find the peak file with the most peaks anymore

```{r}
# now that i have the hlow and scrm that have the most peaks called from the idr pairs
# I will create the master peak file

#peaklist = list(best_hlow_idr, best_scrm_idr )
#peaklist = list('./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak', './idr_results/H3k27me3/Scrm/concat_IDR_Scrm_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak')

#peaklist = list('./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak', './idr_results/H3k27me3/published/concat_IDR_published_H3k27me3_r1_vs_r2_vs_null_0.4_pairs.broadPeak')

# this uses the control k27 cut&tag data from geo instead of Scrm
#peaklist = list('./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak', './idr_results/H3k27me3/control/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak')

# now to use the 10kb merged peaks
peaklist = list('./idr_results/merged_10kb_peaks/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs_10kb_merged.bed', './idr_results/merged_10kb_peaks/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs_10kb_merged.bed')

#best_hlow_idr
#best_scrm_idr

library(GenomicRanges)
library(chromVAR)
library(DESeq2)
library(tidyr)
library(EnhancedVolcano)

# making the master peak genomic ranges object
mPeak = GRanges()

for (peakfile in peaklist) {
  
  # not doing this becasue it will make my coordinate off by 1
  # peaktable = read.table(peakfile, header = FALSE, sep = "\t")
  
  # so now I can comment this out also
  #gr_object = GRanges(seqnames = peaktable$V1, IRanges(start = peaktable$V2, end = peaktable$V3), strand = "*" )
  
  # and add this
  # rtracklayer automatically converts the 0 based bed file system to a 1 based R system
  gr_object = rtracklayer::import(peakfile, format = "BED")
  
  mPeak = append(mPeak, gr_object)
}

# making sure there are no redundant peaks
masterPeak = reduce(mPeak)




```


# now I want to keep the standard chromosomes

```{r}

seqnames_to_keep =masterPeak@seqnames@values[1:23]

masterPeak = masterPeak[seqnames(masterPeak) %in% seqnames_to_keep]

```


# hoping to export my GRanges object master peaks to a bed file

```{r}

rtracklayer::export.bed(masterPeak, con = "10kb_merged_masterPeak_geo_control_h1low.bed")

# rtracklayer does not like labeling the file .broadPeak. so dont do something like this below
# rtracklayer::export.bed(masterPeak, con = "masterPeak_rtracklayer_test.broadPeak")
#rtracklayer::export.bed(masterPeak, )

```




# **SKIP** 
## Now do I use all of the bam files to get counts with the chromVAR get counts package or do I only use the 4 libraries that went into making the idr pair peak files?

# this is me getting the correct bam files 
```{r}

# now lets use chromVAR to load in the counts

# load the bam files separately per condition so i can choose the correct one

hlow_bam_files = list.files(path="./bam_files/", pattern="H1low.*H3k27me3.*bam$", full.names = TRUE )
hlow_bam_files

# now get the tokens from the best hlow file and see if it is in the name of the bam. if it is, we keep the bam

hlow_idr_tokens = strsplit(best_hlow_idr,split = "_")[[1]]
first_rep_hlow = hlow_idr_tokens[5]
second_rep_hlow = hlow_idr_tokens[7]


# make an empty list to store the master bams
list_of_mBams = list()

for (x in hlow_bam_files) {
  
  target_bam = strsplit(x, split = "_")[[1]]
  
  if (first_rep_hlow %in% target_bam) {
    
    print(paste("first rep is here", x))
    
    list_of_mBams = append(list_of_mBams, x)
  }
  if (second_rep_hlow %in% target_bam) {
    
    print(paste("second rep is here", x))
    
    list_of_mBams = append(list_of_mBams, x)
    
  }
  
}

list_of_mBams

# now doing the same for scrm


scrm_bam_files = list.files(path="./bam_files/", pattern="Scrm.*H3k27me3.*bam$", full.names = TRUE )
scrm_bam_files

# getting tokens
scrm_idr_tokens = strsplit(best_scrm_idr,split = "_")[[1]]
first_rep_scrm = scrm_idr_tokens[5]
second_rep_scrm = scrm_idr_tokens[7]


for (x in scrm_bam_files) {
  
  target_bam = strsplit(x, split = "_")[[1]]
  
  if (first_rep_scrm %in% target_bam) {
    
    print(paste("first rep is here", x))
    
    list_of_mBams = append(list_of_mBams, x)
  }
  if (second_rep_scrm %in% target_bam) {
    
    print(paste("second rep is here", x))
    
    list_of_mBams = append(list_of_mBams, x)
    
  }
  
}



```



# doing this for the concat files just use all bams

```{r}

hlow_bam_files = list.files(path="../bam_files/", pattern="H1low.*H3k27me3.*bam$", full.names = TRUE )
#hlow_bam_files

#published_bam_files = list.files(path="./bam_files/", pattern="published.*H3k27me3.*bam$", full.names = TRUE )

# using geo control bam files
#control_bam_files = list.files(path="./nexdep_get_bams/NEXDEP-Nextflow_DNA_Epigenomic_Pipeline/results_PE/sorted_bam_files/blacklist_filt_bam", pattern="control.*H3k27me3.*bam$", full.names = TRUE )
#published_bam_files

# have to actually use our scrambled bams for the counts
scrm_bam_files = list.files(path='../bam_files',  pattern = "Scrm.*H3k27me3.*bam$", full.names = TRUE)

#list_of_mBams = c(hlow_bam_files, control_bam_files)

list_of_mBams = c(hlow_bam_files, scrm_bam_files)

list_of_mBams

# test
#new_mBams = NULL

#for (bam in list_of_mBams) {
  
 # tokens = strsplit(basename(bam), split = "_")[[1]]
  #print(tokens)
  
  #histone = tokens[2]
  #print(histone)
  #if ("H3k27me3" == histone) {
  #  print(bam)
  #  new_mBams = append(new_mBams, bam)
  #}
#}
```


## Now to use chromVAR to fill the matrix


```{r}

# making the matrix 

countsMatrix = matrix(data = NA, length(masterPeak), length(list_of_mBams) )


# I want to get the chr start end of the peaks and put them in the matrix as column names so I know which peaks I am looking at from the deseq2 results in the differential peaks.
seqnames(masterPeak)

# i should put the unique ids from the master peak object in the matrix as row names.
unique_masterPeak_ids = paste0(as.character(seqnames(masterPeak)), ":" ,start(masterPeak), "-", end(masterPeak))

rownames(countsMatrix) = unique_masterPeak_ids

#getting the list of bam base names to add to the matrix column names
list_bam_basenames = list()


# for deseq2 i need the condition design. so hlow and scrm
# then i will find a way to tally how many of each are there so i can automate the rep count
condition_design = list()

type_design = list()

for (x in c(1:length(list_of_mBams))) {
  
  path_bam = list_of_mBams[[x]]
  print(path_bam)
  bam_basename = basename(path_bam)
  bam_tokens = strsplit(basename(path_bam), split = "_")[[1]]
  
  # labeling the important tokens so it is easier to keep track of
  condition = bam_tokens[1]
  histone = bam_tokens[2]
  replicate = bam_tokens[3]
  
  
  # for later parts I need the condition and to know how many times to repeat them
  condition_design = append(condition_design, paste(condition,histone,sep="_"))
  
  
   # also get the replicate too for type design
  type_design = append(type_design, replicate)
  #type_design = append(type_design, paste(histone,replicate, sep="_"))
  
  
  
  # using chromVAR getCounts function
  fragment_counts = getCounts(path_bam, masterPeak, paired= TRUE, by_rg = FALSE, format = "bam")
  
  
  # putting the fragment counts in the column labeled by the bam name
  countsMatrix[,x] = counts(fragment_counts)[,1]
  
  list_bam_basenames = append(list_bam_basenames, bam_basename)
  
}

colnames(countsMatrix) = list_bam_basenames  
```

```{r}

#first removing the low count fragments. only keeping the rows that are above 5 count in total

keep_Rows = which(rowSums(countsMatrix) > 5)

filt_countmatrix = countsMatrix[keep_Rows,]

# now to get the condition_design
condition_counts = table(unlist(condition_design))

# this gives back the names and the counts give back the counts for each of the names
condition_names = names(condition_counts)
condition_num = as.numeric(condition_counts)




# now i can put both lists in to get back the experiment design
condition_factor = factor(rep(condition_names, times=condition_num))


# I want it so the treatment is second in the list here so it is first in the experimental design in deseq2. This way the experimental design will have the diff results going up or down in the treatment vs control

# for treatment I need to make nextflow take the users input so they can relevel this section
treatment = "H1low_H3k27me3"

if (levels(condition_factor)[1] == treatment ) {
  condition_factor = relevel(condition_factor, levels(condition_factor)[2])
}else {
  condition_factor
}
print(condition_factor)

# repeating the above to have another column with type (replicates)
type_counts = table(unlist(type_design))
type_names = names(type_counts)
type_num = as.numeric(type_counts)

#type_factor = factor(rep(type_names, times=type_counts))
type_factor = factor(rep(type_names, times=type_num[1]))

```

# I want to get the idr threshold and use that as input for the file names and other things

```{r}
peak_file = basename(peaklist[[1]])

idr_used = strsplit(peak_file, split = "_")[[1]][10]
idr_used
```



# now for deseq2 workflow
```{r}

# testing complex experimental design where we account for replicate differences in conditions also
#complex_design = ~ condition_factor + type_factor:condition_factor
# this doesn't work in the new version


# now to do the normal deseq2 workflow

dds = DESeqDataSetFromMatrix(countData = filt_countmatrix,
                             colData = DataFrame(condition_factor, type_factor),
                             design = ~ condition_factor)


# using the function on our data
DDS = DESeq(dds)

norm_DDS = counts(DDS, normalized = TRUE) # normalization with respect to the sequencing depth

# adding _norm onto the column names in the normalized matrix
colnames(norm_DDS) = paste0(colnames(norm_DDS), "_norm")


# provides independent filtering using the mean of normalized counts
res = results(DDS, independentFiltering = FALSE, altHypothesis = "greaterAbs")


# this is looking at the differences between the 3 deseq analyzed options
countMatDiff = cbind(filt_countmatrix, norm_DDS, res)

head(countMatDiff)




 # getting the results name and addding to the coef we want to shrink
  experiment_design_name = resultsNames(DDS)[2]
  
  # useful for visualization and ranking of genes or in this case peaks
  resLFC = lfcShrink(DDS, coef= resultsNames(DDS)[2], type = "apeglm")
  
  
  # finding the up and down regulated counts that pass the threshold

  up_reg = resLFC[which(resLFC$padj < 0.05 & resLFC$log2FoldChange >= 1) ,]
  total_up_reg = length(up_reg[,1])
  #total_up_reg
  down_reg = resLFC[which(resLFC$padj < 0.05 & resLFC$log2FoldChange <= -1) ,]
  total_down_reg = length(down_reg[,1])
  total_down_reg
  
  unchanging_reg = resLFC[which(resLFC$padj < 0.05 & resLFC$log2FoldChange <1 & resLFC$log2FoldChange > -1) ,]
  total_unchanging_reg = length(unchanging_reg[,1])
  total_unchanging_reg
  
  resLFC$Label = ifelse(resLFC$padj > 0.05, "not significant", ifelse(abs(resLFC$log2FoldChange) >=1, "|LFC| > 1 & padj < 0.05", "padj < 0.05" ))
  ma_plot_labeled = ggplot(resLFC, aes(x = baseMean, y = log2FoldChange, color=Label))+
    labs(caption = paste("Up reg (green +1 LFC & padj <0.05) = ", total_up_reg, "\n", "Down reg (green -1 LFC & padj < 0.05) = ", total_down_reg), title = experiment_design_name, subtitle = paste("This means the first condition has these points more than in the second condition. The idr threshold for masterPeaks is",idr_used, sep = " "))+
    theme(plot.caption = element_text(size = 30, face = "bold"))+
    scale_colour_manual(values=c("red", "darkgrey", "pink"))+
    scale_x_continuous(trans='log10')+
    ylim(c(min(-max(resLFC$log2FoldChange),min(resLFC$log2FoldChange)), max(max(resLFC$log2FoldChange),-min(resLFC$log2FoldChange))))+
    geom_point(data = resLFC, aes(x = baseMean, y = log2FoldChange), size = 3)
  
  print(ma_plot_labeled)
  
  
  pdf(file = paste(experiment_design_name,"IDR", idr_used,"MA_plot_our_data_counts.png", sep = "_"), width = 10, height = 10)
  
  print(ma_plot_labeled)
  dev.off()
  
  png(filename = paste(experiment_design_name,"IDR", idr_used,"MA_plot_our_data_counts.png", sep = "_"), width = 1000, height = 1000, antialias = "subpixel")
  
  print(ma_plot_labeled)
  dev.off()
  
  
  volcano_plot_removed_reps = EnhancedVolcano(resLFC,
                  lab = rownames(resLFC),
                  title = experiment_design_name, subtitle = paste("This means the first condition has these points more than in the second condition. The idr threshold for masterPeaks is",idr_used, sep = " "),
                  x = 'log2FoldChange', FCcutoff = 1,
                  y = 'padj', pCutoff = 0.05, labSize = 3, 
                  
                  )
  
  print(volcano_plot_removed_reps)
  
  png(filename = paste(experiment_design_name,"IDR",idr_used,"volcano_plot_our_data_counts.png", sep = "_"), width = 1000, height = 1000, antialias = "subpixel")
  print(volcano_plot_removed_reps)
  dev.off()
  
  pdf(file = paste(experiment_design_name,"IDR",idr_used,"volcano_plot_our_data_counts.pdf", sep = "_"), width = 10, height = 10)
  print(volcano_plot_removed_reps)
  dev.off()
  



# using rlog over vst for transformation

rld = rlog(DDS, blind=FALSE)

#it is clear that the rlog transformation inherently accounts for differences in sequencing depth *
head(assay(rld), 5)


library(ggplot2)

pcaData = plotPCA(rld, intgroup=c("condition_factor","type_factor"), returnData = TRUE )
pcaData

percentVar <- round(100 * attr(pcaData, "percentVar"))


pca_plot_rlog = ggplot(pcaData, aes(PC1, PC2, color=condition_factor, shape=type_factor)) +
  ggtitle(paste("PCA plot using Rlog transform IDR", idr_used, sep = " ")) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) + 
  coord_fixed()
#pca_plot
name_for_rlog_png_file =paste(experiment_design_name,"PCA_plot_IDR_our_data_counts", idr_used, "rlog.png", sep = "_")

png(filename = name_for_rlog_png_file, width = 1000, height = 1000, antialias = "subpixel")
print(pca_plot_rlog)
dev.off()

name_for_rlog_pdf_file =paste(experiment_design_name,"PCA_plot_IDR_our_data_counts", idr_used, "rlog.pdf", sep = "_")

pdf(file = name_for_rlog_pdf_file, width = 10, height = 10)
print(pca_plot_rlog)
dev.off()


# testing with vst
vsd_t = vst(DDS, blind = FALSE)
head(assay(vsd_t), 5)

pcaData2 = plotPCA(vsd_t, intgroup=c("condition_factor","type_factor"), returnData = TRUE )
pcaData2

percentVar <- round(100 * attr(pcaData2, "percentVar"))
pca_plot_vst = ggplot(pcaData2, aes(PC1, PC2, color=condition_factor, shape=type_factor)) +
  ggtitle(paste("PCA plot using VST transform IDR", idr_used, sep = " ")) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) + 
  coord_fixed()


#name_for_vst_png_file =paste(experiment_design_name,"PCA_plot_IDR", idr_used, "vst.png", sep = "_")

pdf(file = paste(experiment_design_name,"PCA_plot_IDR_our_data_counts",idr_used,"vst_all_reps.pdf", sep = "_"), width = 10, height = 10)
print(pca_plot_vst)
dev.off()

png(file = paste(experiment_design_name,"PCA_plot_IDR_our_data_counts", idr_used,"vst_all_reps.png", sep = "_"), width = 1000, height = 1000)
print(pca_plot_vst)
dev.off()

pca_plot_rlog

pca_plot_vst


```

# getting the up and down reg peaklist
```{r}

# I can get the row names and use rtracklayer to export the up and down regulated peaks
# but it is subtracting 1 from the start coordinates before making the bed file
# I will have to fix later but currently this is giving me what I want

rtracklayer::export.bed(row.names(up_reg), con = "up_regulated_peaks.bed")

rtracklayer::export.bed(row.names(down_reg), con = "down_regulated_peaks.bed")

# now exporting the unchanging peaks

rtracklayer::export.bed(row.names(unchanging_reg), con = "unchanging_regulated_peaks.bed")
```


# now I want to get the up and down peaks and make a histogram based on their lengths

```{r}

up_peaks = read.table(file = './up_regulated_peaks.bed', header = FALSE, sep = "\t")

up_peak_lengths = up_peaks$V3 - up_peaks$V2
print(max(up_peak_lengths))

png(filename = "hist_up_regulated_peak_lengths.png", height = 1000, width = 1000)
hist.default(up_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))
dev.off()

# just to view it here
hist.default(up_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))

```

# now to get the down peaks

```{r}
down_peaks = read.table(file = './down_regulated_peaks.bed', header = FALSE, sep = "\t")

down_peak_lengths = down_peaks$V3 - down_peaks$V2
print(max(down_peak_lengths))

png(filename = "hist_down_regulated_peak_lengths.png", height = 1000, width = 1000)
hist.default(down_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))
dev.off()

hist.default(down_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))



```


# doing an histogram of the unchanging peak lengths


```{r}

unchanging_peaks = read.table(file = './unchanging_regulated_peaks.bed', header = FALSE, sep = "\t")

unchanging_peak_lengths = unchanging_peaks$V3 - unchanging_peaks$V2
print(max(unchanging_peak_lengths))

png(filename = "hist_unchanging_regulated_peak_lengths.png", height = 1000, width = 1000)
hist.default(unchanging_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))
dev.off()

hist.default(unchanging_peak_lengths, xaxt = "n", breaks = 1e2)
axis(1, at = axTicks(1), labels = format(axTicks(1), scientific = FALSE, big.mark = ","))


```

# try to make a single histogram with up down and unchanging peaks

```{r}

up_peak_df = DataFrame(peak_lengths = up_peak_lengths, category = "up_peaks_lengths")
down_peak_df = DataFrame(peak_lengths = down_peak_lengths, category = "down_peaks_lengths")
unchanging_peak_df = DataFrame(peak_lengths = unchanging_peak_lengths, category = "unchanging_peaks_lengths")


df_all_peak_lengths = rbind(up_peak_df, down_peak_df, unchanging_peak_df)

#df_all_peak_lengths

df_all_peak_lengths_gg =  ggplot(df_all_peak_lengths, aes( category, peak_lengths))
  
df_all_peak_lengths_gg+
  geom_violin()+
  scale_y_continuous(labels = scales::label_number())

```



```{r}


# first I need to make a new dataframe where one column are the down_peak lengths and the next column is the label to show the lengths are from down peaks

# then I use ggplot and pass in that new dataframe and specify the aes to be the names of the two columns

down_peaks_new_df = DataFrame(lengths = down_peak_lengths, condition= rep("down_peaks", length(down_peak_lengths)))

up_peaks_new_df = DataFrame(lengths = up_peak_lengths, condition = rep("up_peaks", length(up_peak_lengths)) )


combined_df = rbind(down_peaks_new_df, up_peaks_new_df)

new_down_peak_gg = ggplot(combined_df, aes(condition, lengths)) 

up_and_down_peak_lengths = new_down_peak_gg+
  geom_violin()+
  scale_y_continuous(labels = scales::label_number())+
  ggtitle("Lengths of down peaks and up peaks")
  #scale_x_continuous(labels = scales::label_number())
  #geom_point()

ggsave("up_and_down_peak_lengths_violin_plot.png")

print(up_and_down_peak_lengths)

```


# so here i should load in the merged 10kb idr peaks for all three conditions (scrm, geo_control, h1low), and get 3 violin plots

```{r}

library(rtracklayer)

# loading h1low
h1low_10kb_merged_peaks = rtracklayer::import("/lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/idr_results/merged_10kb_peaks/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs_10kb_merged.bed")


scrm_10kb_merged_peaks = rtracklayer::import("/lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/idr_results/merged_10kb_peaks/concat_IDR_Scrm_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs_10kb_merged.bed")

geo_control_10kb_merged_peaks = rtracklayer::import("/lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/test_published_data/idr_results/merged_10kb_peaks/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs_10kb_merged.bed")

# then making them into a dataframe

h1low_10kb_merged_peaks_df = as.data.frame(h1low_10kb_merged_peaks)
scrm_10kb_merged_peaks_df = as.data.frame(scrm_10kb_merged_peaks)
geo_control_10kb_merged_peaks_df = as.data.frame(geo_control_10kb_merged_peaks)


# this already has the widths of the peaks
# so I need to add the conditions

h1low_10kb_merged_peaks_df["condition"] = rep("h1low", length(h1low_10kb_merged_peaks_df[,1]))

scrm_10kb_merged_peaks_df["condition"] = rep("scrm", length(scrm_10kb_merged_peaks_df[,1]))

geo_control_10kb_merged_peaks_df["condition"] = rep("geo_control", length(geo_control_10kb_merged_peaks_df[,1]))


# maybe I can cbind all three df


all_conditions_df = bind_rows(h1low_10kb_merged_peaks_df, scrm_10kb_merged_peaks_df, geo_control_10kb_merged_peaks_df)

# so now I need to use the condition column and width column for the aes data

violin_plot_100k_cutoff = ggplot(all_conditions_df, aes(condition, width))+
  geom_violin()+
  scale_y_continuous(labels = scales::label_number(), limits = c(1,100000))+
  ggtitle("Lengths of 10kb merged peaks")

ggsave("violin_plot_10kb_merged_lengths_100k_cutoff.png", plot = violin_plot_100k_cutoff)

print(violin_plot_100k_cutoff)



# plotting the full violin plot without a cutoff

violin_plot_no_cutoff = ggplot(all_conditions_df, aes(condition, width))+
  geom_violin()+
  scale_y_continuous(labels = scales::label_number())+
  ggtitle("Lengths of 10kb merged peaks")

ggsave("violin_plot_10kb_merged_lengths_no_cutoff.png", plot = violin_plot_no_cutoff)

print(violin_plot_no_cutoff)
```



# i can make a ven diagram to show the peaks also and how many overlap in the different conditions


```{r}

# you can see the 3 conditions and how many rows(peaks) they have before merging the geo_control and h1low into master peaks

geo_control = all_conditions_df[all_conditions_df["condition"] == "geo_control",]

scrm = all_conditions_df[all_conditions_df["condition"] == "scrm",]

h1low = all_conditions_df[all_conditions_df["condition"] == "h1low",]

#all_conditions_df

scrm_bool = all_conditions_df["condition"] == "scrm"
all_conditions_df["scrm_bool"]  = all_conditions_df["condition"] == "scrm"

geo_control_bool = all_conditions_df["condition"] == "geo_control"
all_conditions_df["geo_control_bool"]  = all_conditions_df["condition"] == "geo_control"

h1low_bool = all_conditions_df["condition"] == "h1low"
all_conditions_df["h1low_bool"]  = all_conditions_df["condition"] == "h1low"

library(ggVennDiagram)

#rtracklayer:

ggvenn(all_conditions_df, category.names = list(colnames(all_conditions_df)[7:9]))
#VennDiagram::venn.diagram(all_conditions_df[,7:9], filename = "test_venn_diagram.png", imagetype = png,  )
```




# now plotting the lengths on a violin plot

```{r}

length(up_peak_lengths)

length(down_peak_lengths)

down_peak_gg = ggplot(down_peaks, aes(V2, V3)) 

down_peak_gg+
  geom_violin()+
  scale_x_continuous(labels = scales::label_number())+
  scale_y_continuous(labels = scales::label_number())+
  geom_point()

#test_df = DataFrame(up_peak_lengths[1:20], down_peak_lengths[1:20])

#f = ggplot(test_df, aes(up_peak_lengths, down_peak_lengths))

#f+geom_violin(scale = "area")

```

